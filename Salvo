#!/usr/bin/env perl
# Salvo
use strict;
use warnings qw(io);
use feature 'say';
use autodie;
use Getopt::Long;
use IO::Dir;
use File::Find;
use Cwd;

INIT {
    ## environmental variables to work with.
    $ENV{SBATCH} = {
        lonepeak  => '/uufs/lonepeak.peaks/sys/pkg/slurm/std/bin/sbatch',
        ash       => '/uufs/ash.peaks/sys/pkg/slurm/std/bin/sbatch',
        kingspeak => '/uufs/kingspeak.peaks/sys/pkg/slurm/std/bin/sbatch',
        ember     => '/uufs/ember.arches/sys/pkg/slurm/std/bin/sbatch',
    };

    $ENV{SQUEUE} = {
        ash       => '/uufs/ash.peaks/sys/pkg/slurm/std/bin/squeue',
        lonepeak  => '/uufs/lonepeak.peaks/sys/pkg/slurm/std/bin/squeue',
        kingspeak => '/uufs/kingspeak.peaks/sys/pkg/slurm/std/bin/squeue',
        ember     => '/uufs/ember.arches/sys/pkg/slurm/std/bin/squeue',
    };

    $ENV{SINFO} = {
        ash       => '/uufs/ash.peaks/sys/pkg/slurm/std/bin/sinfo',
        lonepeak  => '/uufs/lonepeak.peaks/sys/pkg/slurm/std/bin/sinfo',
        kingspeak => '/uufs/kingspeak.peaks/sys/pkg/slurm/std/bin/sinfo',
        ember     => '/uufs/ember.arches/sys/pkg/slurm/std/bin/sinfo',
    };

    $ENV{SCANCEL} = {
        ash       => '/uufs/ash.peaks/sys/pkg/slurm/std/bin/scancel',
        lonepeak  => '/uufs/lonepeak.peaks/sys/pkg/slurm/std/bin/scancel',
        kingspeak => '/uufs/kingspeak.peaks/sys/pkg/slurm/std/bin/scancel',
        ember     => '/uufs/ember.arches/sys/pkg/slurm/std/bin/scancel',
    };
}

my $usage = <<"EOU";

Synopsis:

    Salvo - Slurm command and job launcher v 0.2.1

    ./Salvo -command_file <FILE> -account <STRING> -partition <STRING> 
    ./Salvo -command_file <FILE> -account <STRING> -partition <STRING> -just_batch 

Description:

    Designed to aid launching of jobs on Slurm cluster from a command list file.
    View github page <https://github.com/srynobio/Salvo> for more detailed description.

    Version 0.2.1 now allows CHPC users to submit jobs to:
        kingspeak-guest
        ash-guest
        lonepeak-guest (currently does not have access to UCGD lustre space).
        ember-guest.

Required options:

    -command_file, -cf      :   File containing list of commands to run. <FILE>
    -account, -a            :   CHPC account name. e.g. yandell-em. <STRING> 
    -partition, -p          :   CHPC partition to run jobs on. e.g. ember-freecycle <STRING>
    -cluster, -c            :   Cluster to launch to.

Options without full requirements:

    -sinfo_idle, -si        :   Prints to STDOUT all idle nodes in all cluster environments.
    -squeue_me, -sm         :   Prints to STDOUT squeue information in all cluster environments under your uid number. 
    -scancel, -cxl          :   Will run the scancel command on requested cluster.  (account, cluster required).

Additional options:

    -uid                    :   Your University or employee id. <STRING> (default ENV{USER} ). 
    -time, -t               :   Time to allow each job to run on node <STRING> (default 1:00:00).
    -node, -n               :   Number of nodes to run per sbatch job submitted. <INT> default 1).
    -queue_limit, -ql       :   Number of jobs to launch and run in the queue at one time. <INT> (default 1).
    -jobs_per_sbatch, -jps  :   Number of jobs to run concurrently, & added to each command. <INT> (default 1);
    -added_steps, -as       :   Additional step to add to each sbatch job <STRING> (comma separated).
    -just_sbatch            :   This option will create all sbatch jobs die, but not submit them (default FALSE).
    -exclude_node, -en      :   List of nodes to exclude (in quotes) in your sbatch script. example: "kp[001-095,168-195,200-227]".
    -chdir                  :   This option will tell each sbatch job to cd into this directory before running command. <STRING> (default current).
    -clean                  :   Option will remove launch.index, *sbatch and *out jobs.
    -help                   :   Prints this battleworn help message.

EOU

my %salvo_opts;
GetOptions(
    \%salvo_opts,
    "commands_file|cf=s",
    "jobs_per_sbatch|jps=i",
    "time|t=s",
    "nodes|n=i",
    "account|a=s",
    "partition|p=s",
    "cluster|c=s",
    "queue_limit|ql=i",
    "clean",
    "just_sbatch",
    "added_steps|as=s",
    "chdir=s",
    "uid=s",
    "squeue_me|sm",
    "scancel|cxl",
    "exclude_node|en=s",
    "sinfo_idle|si",
    "help|h",
);

## set up some defaults.
$salvo_opts{jobs_per_sbatch} //= 1;
$salvo_opts{time} //= '1:00:00';
$salvo_opts{nodes} //= 1;
$salvo_opts{queue_limit} //= 1;
$salvo_opts{uid} //= $ENV{USER};

## Just run clean up.
if ( $salvo_opts{clean} ) { 
    `rm *out *sbatch launch.index`;
    say "Cleaned up!";
    exit(0);
}

## Options without full requirements:
if ( $salvo_opts{sinfo_idle} ) {
    sinfo_idle();
    exit(0);
}

if ( $salvo_opts{squeue_me} ) {
    squeue_me();
    exit(0);
}

if ( $salvo_opts{scancel} ) {
    unless ( $salvo_opts{account} and $salvo_opts{cluster} ) {
        die $usage, "-account, -cluster required!";
    }
    scancel();
    exit(0);
}

## Require check
unless ( 
    $salvo_opts{commands_file} and $salvo_opts{account} and $salvo_opts{cluster} and $salvo_opts{partition} ) {
    die "$usage\n[ERROR] - Options command_file, account, cluster and partition required.";
}

## clean up old sbatch scripts.
say "Looking for and removing any prior sbatch scripts";
my @found_sbatch = `find . -name \"*sbatch\"`;
chomp @found_sbatch;
if ( @found_sbatch ) {
    say "removing found sbatch scripts";
    map { `rm $_` } @found_sbatch;
}

## set up some defaults.
$salvo_opts{jobs_per_sbatch} //= 1;
$salvo_opts{time} //= '1:00:00';
$salvo_opts{nodes} //= 1;
$salvo_opts{queue_limit} //= 1;

## if step were added get them ready.
my @steps;
if ($salvo_opts{added_steps}) {
    if ($salvo_opts{added_steps} =~ /\,/) {
        @steps = split /\,/, $salvo_opts{added_steps};
    }
    else {
        push @steps, $salvo_opts{added_steps};
    }
}

## Get the supplied dir or the current working.
my $dir = (($salvo_opts{chdir}) ? $salvo_opts{chdir} : getcwd);

## open command file
my $CMDS = IO::File->new($salvo_opts{commands_file});

my @cmds;
foreach my $cmd (<$CMDS>) {
    chomp $cmd;
    $cmd =~ s/$/ &/ if ($salvo_opts{jobs_per_sbatch} > 1);
    push @cmds, $cmd;
}
$CMDS->close;

## split base on jps, then create sbatch scripts.
my @var;
push @var, [ splice @cmds, 0, $salvo_opts{jobs_per_sbatch} ] while @cmds;

my $jobid = 1;
foreach my $group (@var) {
    chomp $group;
    $jobid++;
    writer($group, $jobid);
}

## just create sbatch jobs dont submit.
die "[WARN] - sbatch scripts written but not submitted\n" if ($salvo_opts{just_sbatch});

## Review total sbatch to launch.
my @total  = glob "*sbatch";
chomp @total;
my $number_of_jobs = scalar @total;
say "[WARN]  - sbatch submission total: $number_of_jobs";
say "[INPUT] - continue? [y/n]";
my $continue = <STDIN>;
chomp $continue;
($continue eq 'y') ? say "Salvo running..." : exit(0);

## submit sbatch jobs.
my $DIR = IO::Dir->new(".");
my $running = 0;
foreach my $launch ($DIR->read) {
    chomp $launch;
    next unless ( $launch =~ /sbatch$/);

    if ( $running >= $salvo_opts{queue_limit} ) {
        my $status = _jobs_status();
        if ( $status eq 'add' ) {
            $running--;
            redo;
        }
        elsif ( $status eq 'wait' ) {
            sleep(10);
            redo;
        }
    }
    else {
        system "$ENV{SBATCH}->{$salvo_opts{cluster}} $launch &>> launch.index";
        $running++;
        next;
    }
}

# give sbatch system time to work
sleep(10);

# check the status of current sbatch jobs
# before moving on.

_wait_all_jobs();
_error_check();

## --------------------------------------------------- ##

sub _jobs_status {
    my $state = `$ENV{SQUEUE}->{$salvo_opts{cluster}} -A $salvo_opts{account} -u $salvo_opts{uid} -h | wc -l`;

    if ( $state >= $salvo_opts{queue_limit} ) {
        return 'wait';
    }
    else {
        return 'add';
    }
}

## --------------------------------------------------- ##

sub _wait_all_jobs {

    my $process;
    do {
        sleep(60);
        _relaunch();
        sleep(60);
        $process = _process_check();
    } while ($process);
}

## --------------------------------------------------- ##

sub _relaunch {

    my @error;
    if ( -e '*.out' ) {
        @error = `grep error *.out`;
        chomp @error;
    }
    if ( !@error ) { return }

    my @relaunch;
    my @error_files;
    foreach my $cxl (@error) {
        chomp $cxl;

        my @ids = split /\s/, $cxl;
        my ( $error_file, undef ) = split /:/, $ids[0];

        ## add to error array before changes
        push @error_files, $error_file;

        ## rename as sbatch script 
        ( my $sbatch = $error_file) =~ s/out/sbatch/;

        if ( $cxl =~ /TIME LIMIT/ ) {
            say "[WARN] $sbatch was cancelled due to time limit";
            next;
        };

        ## record launch id with sbatch script
        next unless ( $cxl =~ /PREEMPTION|CANCELLED/ );
        push @relaunch, $sbatch;
    }

    foreach my $rerun (@relaunch) {
        chomp $rerun;
        system "$ENV{SBATCH}->{$salvo_opts{cluster}} $rerun >> launch.index";
        say "[WARN] Relaunching job $rerun";
    }

    ## remove error files.
    unlink @error_files;
}

## --------------------------------------------------- ##

sub _process_check {

    my @processing = `$ENV{SQUEUE}->{$salvo_opts{cluster}} -A $salvo_opts{account} -u $salvo_opts{uid} -h --format=%A`;
    chomp @processing;
    if ( !@processing ) { return 0 }

    ## check run specific processing.
    ## make lookup of what is running.
    my %running;
    foreach my $active ( @processing ) {
        chomp $active;
        $active =~ s/\s+//g;
        $running{$active}++;
    }

    ## check what was launched.
    open(my $LAUNCH, '<', 'launch.index') or die "[ERROR] Can't find needed launch.index file.";

    my $current = 0;
    foreach my $launched ( <$LAUNCH> ) {
        chomp $launched;
        my @result = split /\s+/, $launched;

        if ( $running{$result[-1]} ) {
            $current++;
        }
    }
    ($current) ? (return 1) : (return 0);
}

## --------------------------------------------------- ##

sub _error_check {
    my @error = `grep error *.out`;
    if ( !@error ) { return }

    if (@error) {
        say "[WARN] Errors found relaunching jobs.";
        _wait_all_jobs();
    }

    say "[WARN] errors were found in salvo output files", 
    map { say "[WARN] Output File: $_" } @error;
}

## --------------------------------------------------- ##

sub squeue_me {
    foreach my $cluster ( keys %{ $ENV{SQUEUE} } ) {
        system("$ENV{SQUEUE}->{$cluster} -u $salvo_opts{uid} -h ");
    }
}

## --------------------------------------------------- ##

sub sinfo_idle {
    foreach my $cluster ( keys %{ $ENV{SINFO} } ) {
        system("$ENV{SINFO}->{$cluster} | grep idle");
    }
}

## --------------------------------------------------- ##

sub scancel {
    my $cluster = $salvo_opts{cluster};
    say "[WARN] canceling jobs on $cluster";
    system("$ENV{SCANCEL}->{$cluster} -A $salvo_opts{account} -u $salvo_opts{uid}");
}

## --------------------------------------------------- ##

sub writer {
    my ($stack, $jobid) = @_;

    my $jobname = 'salvo-' . $jobid;
    my $slurm_out = $jobname . '.out';
    my $outfile = $jobname . '.sbatch';

    my $cmds = join("\n", @{$stack} );
    my $extra_steps = join("\n", @steps) if @steps;

    ## add the exclude option
    my $exclude;
    if ($salvo_opts{exclude_node} ) {
        $exclude = "#SBATCH -x $salvo_opts{exclude_node}";
    }
    else {
        $exclude = '';
    }

    my $sbatch = <<"EOM";
#!/bin/bash
#SBATCH -t $salvo_opts{time}
#SBATCH -N $salvo_opts{nodes}
#SBATCH -A $salvo_opts{account}
#SBATCH -p $salvo_opts{partition}
#SBATCH -J $jobname
#SBATCH -o $slurm_out
$exclude

cd $dir

$extra_steps

$cmds

wait

EOM

open(my $OUT, '>', $outfile);
say $OUT $sbatch;

close $OUT;
}

## --------------------------------------------------- ##

