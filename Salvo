#!/usr/bin/env perl
# Salvo2
use strict;
use warnings;
use feature 'say';
use autodie;
use FindBin;
use lib "$FindBin::Bin/lib";
use Salvo;
use Getopt::Long;

my $usage = <<"EOM"; 

Synopsis:

    Salvo - Slurm command and job launcher v 1.2.2

Description:

    Designed to aid launching of Slurm jobs from a command list file.
    View github page <https://github.com/srynobio/Salvo> for more detailed description.

    Version 1.2.1 now allows CHPC users to submit jobs to:
    kingspeak-guest
    ash-guest
    ember-guest
    lonepeak-guest : (lonepeak does not have access to UCGD lustre space).

    Salvo can be launched in two distinct modes: dedicated & idle.  
    Options and differences are given below.


Dedicated:

    Dedicated required options:

    -command_file, -cf      :   File containing list of commands to run. <FILE>
    -account, -a            :   CHPC account name. e.g. yandell-em. <STRING>
    -partition, -p          :   CHPC partition to run jobs on. e.g. ember-freecycle <STRING>
    -cluster, -c            :   Cluster to launch to. e.g. ash <STRING>
    -mode, -m               :   Launch mode. e.g.  dedicated


    Additional options:

    -runtime, -r            :   Time to allow each job to run on node. <STRING> (default 5:00:00)
    -nodes_per_sbatch, -nps :   Number of nodes to run per sbatch launch <INT> (default 1)
    -jobs_per_sbatch, -jps  :   Number of jobs to run per sbatch launch. <INT> (default 1)
    -queue_limit, -ql       :   Number of jobs queue/run per cluster at one time. <INT> (default 1)
    -additional_steps, -as  :   Additional step to add to each sbatch job <STRING> (comma separated)
    -work_dir, -wd          :   This option will add the directory to work out of to each sbatch job. <STRING> (default current)
    -exclude_nodes, -en     :   Will exclude submission to selected nodes <STRING> e.g. kp[001-095,168-195,200-227]
    -jobname, -j            :   Jobnames to give to a current launch. <STRING> (default salvo)
    -concurrent             :   Will add "&" to the end of each command allowing concurrent runs.


Idle:

    Idle required options:

    -command_file, -cf      :   File containing list of commands to run. <FILE>
    -mode, -m               :   Launch mode. e.g. idle


    Additional options:

    -runtime, -r            :   Time to allow each job to run on node. <STRING> (default 5:00:00)
    -nodes_per_sbatch, -nps :   Number of nodes to run per sbatch launch <INT> (default 1)
    -jobs_per_sbatch, -jps  :   Number of jobs to run per sbatch launch. <INT> (default 1)
    -queue_limit, -ql       :   Number of jobs queue/run per cluster at one time. <INT> (default 50)
    -additional_steps, -as  :   Additional step to add to each sbatch job <STRING> (comma separated)
    -work_dir, -wd          :   This option will add the directory to work out of to each sbatch job. <STRING> (default current)
    -exclude_cluster, -ec   :   Will exclude submission to select cluster. <STRING> e.g. lonepeak
    -exclude_nodes, -en     :   Will exclude submission to selected nodes <STRING> e.g. kp[001-095,168-195,200-227]
    -jobname, -j            :   Jobname to give to current launch. <STRING> (default salvo)
    -concurrent             :   Will add "&" to the end of each command allowing concurrent runs.
    -hyperthread            :   Will read the number of available cpus and double value (should only be used on known hyperthreaded machines).


Additional help options:

    -help                   :   Prints this battleworn help message.
    -clean                  :   Will remove processing, launched, out, complete, cmds files created by Salvo.


Node information options:

    -squeue_me, -sm         : Will output all current runing jobs across all clusters.
    -sinfo_idle, -si        : Will output all currently idle nodes across all clusters.
    -node_info, ni          : Will give a greater detailed output of all idle nodes across all clusters.
    -reserve_info, ri       : Will give detailed output of all current reserves across all clusters.

EOM

my %salvo_opts;
GetOptions(
    \%salvo_opts,             "command_file|cf=s",
    "account|a=s",            "partition|p=s",
    "cluster|c=s",            "mode|m=s",
    "jobs_per_sbatch|jps=i",  "reserve_info|ri",
    "hyperthread",            "clean",
    "exclude_cluster|ec=s",   "runtime|r=s",
    "nodes_per_sbatch|nps=i", "queue_limit|ql=i",
    "additional_steps|as=s",  "exclude_nodes|en=s",
    "work_dir|wd=s",
    "jobname|j=s", 
    "concurrent",
    "squeue_me|sm", 
    "sinfo_idle|si",
    "node_info|ni",
    "help|h",
);

if ( $salvo_opts{help} ) {
    say $usage;
    exit(0);
}

## Reporting options.
if ( $salvo_opts{sinfo_idle} ) {
    my $salvo = Salvo->new( \%salvo_opts );
    $salvo->sinfo_idle;
    exit(0);
}
if ( $salvo_opts{squeue_me} ) {
    my $salvo = Salvo->new( \%salvo_opts );
    $salvo->squeue_me;
    exit(0);
}
if ( $salvo_opts{node_info} ) {
    my $salvo = Salvo->new( \%salvo_opts );
    $salvo->report_node_info;
    exit(0);
}
if ( $salvo_opts{reserve_info} ) {
    my $salvo = Salvo->new( \%salvo_opts );
    $salvo->reserve_info;
    exit(0);
}

## Clean up option.
if ( $salvo_opts{clean} ) {
    my @salvo_files = glob q(*processing *launched *out *complete *cmds *error);
    unlink @salvo_files if @salvo_files;
    say "Cleaned up!";
    exit(0);
}

## Check required.
unless ( $salvo_opts{mode} and $salvo_opts{command_file} ) {
    say $usage;
    say "Required options not met.";
    exit(1);
}

my $salvo = Salvo->new( \%salvo_opts );
$salvo->fire;

